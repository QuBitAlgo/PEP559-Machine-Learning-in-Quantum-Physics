# Assignment #2

## Question 1: Understanding Backpropagation in MLP  
Consider a Multi-Layer Perceptron (MLP) model with one input layer, one hidden layer, and one output layer, each consisting of **two** neurons. There is only one sample (no batch dimension) 

Using the same notations introduced in the notes, derive and simplify the following partial derivatives:  

$$
\frac{\partial L}{\partial w_{2,1}^{(out)}}, \quad
\frac{\partial L}{\partial w_{2,2}^{(out)}}, \quad
\frac{\partial L}{\partial b_1^{(out)}}, \quad
\frac{\partial L}{\partial w_{1,2}^{(h)}}, \quad
\frac{\partial L}{\partial b_2^{(h)}}
$$

**Assessment**: Submission is not required. At the beginning of the next lecture, you will need to derive and discuss your result on the whiteboard for one randomly selected term.
